{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Translation Model (before Ray Serve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour monde!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self):\n",
    "        # Load model\n",
    "        self.model = pipeline(\"translation_en_to_fr\", model=\"t5-small\", device='cuda:0')\n",
    "\n",
    "    def translate(self, text: str) -> str:\n",
    "        # Run inference\n",
    "        model_output = self.model(text)\n",
    "\n",
    "        # Post-process output to return only the translation text\n",
    "        translation = model_output[0][\"translation_text\"]\n",
    "\n",
    "        return translation\n",
    "\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "translation = translator.translate(\"Hello world!\")\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to a Ray Serve Application\n",
    "\n",
    "* Run [serve_trans.py](scaling/ray/serve/serve_trans.py)\n",
    "    ```bash\n",
    "    serve run serve_deployment:translator_app\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a post request w/ json data\n",
    "import requests\n",
    "\n",
    "english_text = \"Hello world!\"\n",
    "\n",
    "response = requests.post(\"http://127.0.0.1:8000/\", json=english_text)\n",
    "french_text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error, traceback: \u001b[36mray::ServeReplica:default:MostBasicIngress.handle_request_with_rejection()\u001b[39m (pid=2472, ip=127.0.0.1)\n",
      "  File \"d:\\Miniconda\\envs\\mle_proj\\Lib\\site-packages\\ray\\serve\\_private\\utils.py\", line 168, in wrap_to_ray_error\n",
      "    raise exception\n",
      "  File \"d:\\Miniconda\\envs\\mle_proj\\Lib\\site-packages\\ray\\serve\\_private\\replica.py\", line 1151, in call_user_method\n",
      "    await self._call_func_or_gen(\n",
      "  File \"d:\\Miniconda\\envs\\mle_proj\\Lib\\site-packages\\ray\\serve\\_private\\replica.py\", line 875, in _call_func_or_gen\n",
      "    result = await result\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"D:\\TEMP\\Temp\\ipykernel_17528\\3284836380.py\", line 10, in __call__\n",
      "TypeError: string indices must be integers, not 'str'.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(french_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Multiple Models\n",
    "\n",
    "* Ray Serve allows you to compose multiple deployments into a single Ray Serve application. \n",
    "    * This makes it easy to combine multiple machine learning models along with business logic to serve a single request.\n",
    "    * We can use parameters like `autoscaling_config`, `num_replicas`, `num_cpus` and `num_gpus` to independently configure and scale each deployment in the application.\n",
    "\n",
    "* Example, in the CLI:\n",
    "    * Run [APP](summary_serve_composed.py):\n",
    "        * `serve run summary_serve_composed:app`\n",
    "    * Then run [CLIENT](summary_client.py):\n",
    "        * `python summary_client.py`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop and Deploy an ML Application\n",
    "\n",
    "* The flow for developing a Ray Serve application locally and deploying it in production covers the following steps:\n",
    "    * Converting a Machine Learning model into a Ray serve application\n",
    "    * Testing the application locally\n",
    "    * Building Serve config files for production deployment\n",
    "    * Deploying application using a config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a model into a Ray Serve Application\n",
    "\n",
    "* See [model_serve.py](<deploy ml app/model_serve.py>)\n",
    "* Run in CLI: `serve run model_serve:translator_app`\n",
    "* To send a POST request w/ JSON data, run `python model_client.py`\n",
    "* To check the status of the application and deployment , run `serve status`\n",
    "\n",
    "### Build Serve config files for production deployment\n",
    "* To deploy Serve applications in production, you need to generate a Serve config YAML file. \n",
    "* A Serve config file is the single source of truth for the cluster, allowing you to specify system-level configurations and your applications in one place.\n",
    "    * It also allows you to declaratively update your applications. \n",
    "* The `serve build` CLI command takes as input the import path and saves to an output file using the -o flag.\n",
    "    * `serve build model_serve:translator_app -o config.yaml`\n",
    "        * The `runtime_env` field will always be empty when using `serve build` and must be set manually.\n",
    "    * The `serve build` command adds a default application name that can be modified.\n",
    "    * You can also use the Serve config file with `serve run` for local testing.\n",
    "\n",
    "### Dynamically change parameters without restarting replicas (`user_config`)\n",
    "\n",
    "* You can use the `user_config` field in the YAML to supply a structured configuration for your deployment.\n",
    "    * You can pass arbitrary JSON serializable objects to the YAML configuration.\n",
    "    * Serve then applies it to all running and future deployment replicas.\n",
    "    * The application of user configuration doesn't restart the replica.\n",
    "\n",
    "* This deployment continuity means that you can use this field to dynamically:\n",
    "    * Adjust model weights and versions without restarting the cluster.\n",
    "    * Adjust traffic splitting percentage for your model composition graph.\n",
    "    * Configure any feature flag, A/B tests, and hyper-parameters for your deployments.\n",
    "\n",
    "* To enable the `user_config` feature, implement a `reconfigure` method that takes a JSON-serializable object (e.g., a Dictionary, List, or String) as its only argument\n",
    "\n",
    "* SEE [Updating Applications In-Place](https://docs.ray.io/en/latest/serve/advanced-guides/inplace-updates.html#serve-inplace-updates)\n",
    "\n",
    "\n",
    "* Example:\n",
    "```python\n",
    "    # add to model_serve.py type files\n",
    "    @serve.deployment\n",
    "    class Model:\n",
    "        def reconfigure(self, config: Dict[str, Any]):\n",
    "            self.threshold = config[\"threshold\"]\n",
    "```\n",
    "* The corresponding YAML snippet:\n",
    "```YAML\n",
    "    ...\n",
    "    deployments:\n",
    "        -name: Model\n",
    "        user_config:\n",
    "            threshold: 1.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Compositions of Models\n",
    "\n",
    "### Compose deployments using `DeploymentHandles`\n",
    "\n",
    "* When building an application, you can `.bind()` multiple deployments and pass them to each other's constructors. At runtime, inside the deployment code Ray Serve substitutes the bound deployments w/ `DeploymentHandles` that you can use to call methods of other deployments.\n",
    "* This capability lets you divide your application’s steps, such as preprocessing, model inference, and post-processing, into independent deployments that you can independently scale and configure.\n",
    "* Use `handle.remote` to send requests to a deployment.\n",
    "    * These requests can contain ordinary Python args and kwargs, which DeploymentHandles can pass directly to the method. \n",
    "    * The method call returns a `DeploymentResponse` that represents a future to the output. \n",
    "    * You can `await` the response to retrieve its result or pass it to another downstream `DeploymentHandle` call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining `DeploymentHandle` calls\n",
    "\n",
    "* Ray Serve can directly pass the `DeploymentResponse` object that a `DeploymentHandle` returns, to another `DeploymentHandle` call to chain together multiple stages of a pipeline. \n",
    "    * You don’t need to `await` the first response, Ray Serve manages the `await` behavior under the hood. \n",
    "    * When the first call finishes, Ray Serve passes the output of the first call, instead of the `DeploymentResponse` object, directly to the second call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming `DeploymentHandle` Calls\n",
    "\n",
    "* You can also use `DeploymentHandles` to make streaming method calls that return multiple outputs.\n",
    "* To make a streaming call :\n",
    "    * The method must be a generator and you must set `handle.options(stream=True)`\n",
    "    * Then, the handle call returns a `DeploymentResponseGenerator` instead of a unary `DeploymentResponse`\n",
    "    * You can use `DeploymentResponseGenerators` as a sync or async generator, like in an `async for` code block.\n",
    "    * You can't pass `DeploymentResponseGenerators` to other handle calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Multiple Applications\n",
    "\n",
    "### When to use multiple applications\n",
    "* Suppose you have multiple models and/or business logic that all need to be executed for a single request.\n",
    "\n",
    "* Model Compostion\n",
    "    * If they are living in one repository, then you most likely upgrade them as a unit, so have all those deployments in one application.\n",
    "\n",
    "* Multiple Applications\n",
    "    *  If these models or business logic have logical groups, for example, groups of models that communicate with each other but live in different repositories, separate the models into applications.\n",
    "    * Another common use-case for multiple applications is separate groups of models that may not communicate with each other, but you want to co-host them to increase hardware utilization.\n",
    "    * Because one application is a unit of upgrade, having multiple applications allows you to deploy many independent models (or groups of models) each behind different endpoints. You can then easily add or delete applications from the cluster as well as upgrade applications independently of each other.\n",
    "\n",
    "* See [Example](<deploy multiple applications>):\n",
    "    * Run `serve build imgC:app txtT:app -o config.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAPI and HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
