{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'D:\\Notes\\git local repo\\Diabetes Health Indicator\\data\\diabetesHI\\diabetes_012_health_indicators_BRFSS2015.csv')\n",
    "\n",
    "def sampler_pipeline(df:pd.DataFrame, sampler:str) -> pd.DataFrame:\n",
    "    X = df.drop('Diabetes_012', axis=1)\n",
    "    y = df[['Diabetes_012']]\n",
    "    \n",
    "    sampling_list = ['No resampling (Original Data)', 'imblearn: RandomUnderSampler', 'imblearn: SMOTE']\n",
    "    if sampler == sampling_list[0]:\n",
    "        #return df\n",
    "        return X, y\n",
    "    elif sampler == sampling_list[1]:\n",
    "        ran_down = RandomUnderSampler(random_state=42)\n",
    "        X_dws, y_dws = ran_down.fit_resample(X,y)\n",
    "        #return pd.merge(left=X_dws, right=y_dws, left_index=True, right_index=True)\n",
    "        return X_dws, y_dws\n",
    "    elif sampler == sampling_list[2]:\n",
    "        smote_ups = SMOTE(random_state=42)\n",
    "        X_ups, y_ups = smote_ups.fit_resample(X, y)\n",
    "        #return pd.merge(left=X_ups, right=y_ups, left_index=True, right_index=True)\n",
    "        return X_ups, y_ups\n",
    "    else:\n",
    "        raise IndexError(\"Sampler is not on the pre-defined Sampling List: ['No resampling (Original Data)', 'imblearn: RandomUnderSampler', 'imblearn: SMOTE']\")\n",
    "    \n",
    "\n",
    "X, y = sampler_pipeline(df = df, sampler='imblearn: RandomUnderSampler')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model From MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "uri = \"http://localhost:5000\"\n",
    "mlflow.set_tracking_uri(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(experiment_name):\n",
    "    if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcf8794822d4f52a0638839f7eb06a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/27 20:03:51 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - cloudpickle (current: 2.2.1, required: cloudpickle==3.0.0)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    }
   ],
   "source": [
    "model_uri = 'runs:/1e3489dcb6224eba98e3cee11f078a0a/model_gbc_optuna_2'\n",
    "#gbc_optuna_model = mlflow.sklearn.load_model(model_uri=model_uri)\n",
    "\n",
    "#pyfunc_path = \"/tmp/gbc_dynamic\"\n",
    "gbc_dynamic = mlflow.pyfunc.load_model(model_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 53.33%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = gbc_dynamic.predict(X_test)\n",
    "asc = accuracy_score(y_true=y_test.values.ravel(), y_pred=y_pred)\n",
    "print(f'Accuracy Score: {asc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Another Model Using Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, recall_score, precision_score\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback, setup_mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MLflowLoggerCallback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-27 20:06:45</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:34.16        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.4/15.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None<br>Logical resource usage: 4.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  max_features</th><th style=\"text-align: right;\">  min_samples_leaf</th><th style=\"text-align: right;\">  min_samples_split</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  subsample</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  log_loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  f1_score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_gbc_cb_74d9ee8f</td><td>TERMINATED</td><td>127.0.0.1:37184</td><td style=\"text-align: right;\">      0.0160963</td><td style=\"text-align: right;\">         13</td><td style=\"text-align: right;\">       0.82092</td><td style=\"text-align: right;\">                11</td><td style=\"text-align: right;\">                 17</td><td style=\"text-align: right;\">           494</td><td style=\"text-align: right;\">   0.348102</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         145.537</td><td style=\"text-align: right;\">   1.03304</td><td style=\"text-align: right;\">  0.489203</td><td style=\"text-align: right;\">  0.487969</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 20:04:11,533\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_20-03-56_039243_4828\\artifacts\\2024-11-27_20-04-05\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_cb_74d9ee8f_1_learning_rate=0.0161,max_depth=13,max_features=0.8209,min_samples_leaf=11,min_samples_split=17,n_estimator_2024-11-27_20-04-11\n",
      "2024-11-27 20:04:11,541\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_20-03-56_039243_4828\\artifacts\\2024-11-27_20-04-05\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_cb_74d9ee8f_1_learning_rate=0.0161,max_depth=13,max_features=0.8209,min_samples_leaf=11,min_samples_split=17,n_estimator_2024-11-27_20-04-11\n",
      "2024-11-27 20:04:17,620\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_20-03-56_039243_4828\\artifacts\\2024-11-27_20-04-05\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_cb_74d9ee8f_1_learning_rate=0.0161,max_depth=13,max_features=0.8209,min_samples_leaf=11,min_samples_split=17,n_estimator_2024-11-27_20-04-11\n",
      "2024-11-27 20:04:17,622\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_20-03-56_039243_4828\\artifacts\\2024-11-27_20-04-05\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_cb_74d9ee8f_1_learning_rate=0.0161,max_depth=13,max_features=0.8209,min_samples_leaf=11,min_samples_split=17,n_estimator_2024-11-27_20-04-11\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m 2024/11/27 20:06:41 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m     - be included in your $PATH\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m     - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m \n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m \n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m This initial message can be silenced or aggravated in the future by setting the\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m     - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m     - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m     - error|e|exception|raise|r|2: for a raised exception\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m \n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m Example:\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m \n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m 2024/11/27 20:06:43 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: D:\\TEMP\\Temp\\tmp00r17h_a\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.5.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[36m(train_gbc_cb pid=37184)\u001b[0m 2024/11/27 20:06:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024-11-27 20:06:45,471\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_20-03-56_039243_4828\\artifacts\\2024-11-27_20-04-05\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_cb_74d9ee8f_1_learning_rate=0.0161,max_depth=13,max_features=0.8209,min_samples_leaf=11,min_samples_split=17,n_estimator_2024-11-27_20-04-11\n",
      "2024-11-27 20:06:45,477\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 2.311 s, which may be a performance bottleneck.\n",
      "2024-11-27 20:06:45,478\tWARNING util.py:201 -- The `process_trial_result` operation took 2.312 s, which may be a performance bottleneck.\n",
      "2024-11-27 20:06:45,479\tWARNING util.py:201 -- Processing trial results took 2.313 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2024-11-27 20:06:45,480\tWARNING util.py:201 -- The `process_trial_result` operation took 2.314 s, which may be a performance bottleneck.\n",
      "2024/11/27 20:06:45 INFO mlflow.tracking._tracking_service.client: üèÉ View run train_gbc_cb_74d9ee8f at: http://localhost:5000/#/experiments/637376423660888449/runs/c47314cb49c74c28bfcd673866fe54a1.\n",
      "2024/11/27 20:06:45 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/637376423660888449.\n",
      "2024-11-27 20:06:45,690\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/JCA/ray_results/gradient_boosting_tuning' in 0.0170s.\n",
      "2024-11-27 20:06:45,715\tINFO tune.py:1041 -- Total run time: 160.30 seconds (154.15 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "            'learning_rate': tune.uniform(0.001, 0.1),\n",
    "            'max_depth': tune.randint(1, 15),\n",
    "            'max_features': tune.uniform(0, 1),\n",
    "            'min_samples_leaf': tune.randint(10, 15),\n",
    "            'min_samples_split': tune.randint(15, 18),\n",
    "            'n_estimators':  tune.randint(300, 500),\n",
    "            'subsample':  tune.uniform(0.1, 0.9)\n",
    "        }\n",
    "\n",
    "search_algo = OptunaSearch()\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "artifact_path='model'\n",
    "\n",
    "def train_gbc_cb(config: dict, X_train: pd.DataFrame, y_train: pd.DataFrame):\n",
    "    X_train, y_train = X_train, y_train\n",
    "\n",
    "    gbc_clf = GradientBoostingClassifier(\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_depth=config['max_depth'],\n",
    "        max_features=config['max_features'],\n",
    "        min_samples_leaf=config['min_samples_leaf'],\n",
    "        min_samples_split=config['min_samples_split'],\n",
    "        n_estimators=config['n_estimators'],\n",
    "        subsample=config['subsample']\n",
    "    )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    y_pred_proba = cross_val_predict(estimator=gbc_clf, \n",
    "                                X= X_train, \n",
    "                                y = y_train.values.ravel(), \n",
    "                                cv=skf, \n",
    "                                method='predict_proba')\n",
    "    \n",
    "    log_loss_score = log_loss(y_true=y_train, y_pred=y_pred_proba)\n",
    "    y_pred_class = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "    metrics = {\n",
    "            'log_loss' : log_loss_score,\n",
    "            'accuracy': accuracy_score(y_train, y_pred_class),\n",
    "            'f1_score': f1_score(y_train, y_pred_class, average='macro'),\n",
    "            'recall_score': recall_score(y_train, y_pred_class, average='macro'),\n",
    "            'precision': precision_score(y_train, y_pred_class, average='macro')\n",
    "        }\n",
    "\n",
    "    # Does not work on callback\n",
    "    mlflow.sklearn.log_model(\n",
    "        gbc_clf, artifact_path=artifact_path\n",
    "    )\n",
    "\n",
    "    train.report(\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "\n",
    "train_gbc_gpu_cb = tune.with_resources(train_gbc_cb, {'cpu':4, 'gpu':1})\n",
    "def tune_to_mlflow_cb(mlflow_uri=uri):\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "    `MLflowLoggerCallback()`:\n",
    "        - cannot create \"run groups\" inside an experiment\n",
    "        - cannot log model\n",
    "    \"\"\"\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_parameters(trainable=train_gbc_gpu_cb, X_train=X_train, y_train=y_train),\n",
    "        param_space=search_space,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=1, #10\n",
    "            metric='log_loss', \n",
    "            mode='min',\n",
    "            search_alg=search_algo,\n",
    "            scheduler=scheduler),\n",
    "        run_config=train.RunConfig(\n",
    "            name=\"gradient_boosting_tuning\",\n",
    "            callbacks=[\n",
    "                MLflowLoggerCallback(\n",
    "                    tracking_uri=mlflow_uri,\n",
    "                    experiment_name='DHI_Classifiers_Ray',\n",
    "                    save_artifact=True,\n",
    "                    #tags={\"mlflow.runName\":\"optuna_asha\"}\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "tune_to_mlflow_cb(mlflow_uri=uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `setup_mflow()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 20:06:45,906\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_20-03-56_039243_4828\\artifacts\\2024-11-27_20-06-45\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b2db8e73_1_learning_rate=0.0292,max_depth=13,max_features=0.0821,min_samples_leaf=14,min_samples_split=15,n_estimator_2024-11-27_20-06-45\n",
      "2024-11-27 20:06:45,912\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_20-03-56_039243_4828\\artifacts\\2024-11-27_20-06-45\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b2db8e73_1_learning_rate=0.0292,max_depth=13,max_features=0.0821,min_samples_leaf=14,min_samples_split=15,n_estimator_2024-11-27_20-06-45\n",
      "2024-11-27 20:06:51,497\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_20-03-56_039243_4828\\artifacts\\2024-11-27_20-06-45\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b2db8e73_1_learning_rate=0.0292,max_depth=13,max_features=0.0821,min_samples_leaf=14,min_samples_split=15,n_estimator_2024-11-27_20-06-45\n",
      "2024-11-27 20:06:51,498\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_20-03-56_039243_4828\\artifacts\\2024-11-27_20-06-45\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b2db8e73_1_learning_rate=0.0292,max_depth=13,max_features=0.0821,min_samples_leaf=14,min_samples_split=15,n_estimator_2024-11-27_20-06-45\n",
      "2024-11-27 20:08:01,329\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_20-03-56_039243_4828\\artifacts\\2024-11-27_20-06-45\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b2db8e73_1_learning_rate=0.0292,max_depth=13,max_features=0.0821,min_samples_leaf=14,min_samples_split=15,n_estimator_2024-11-27_20-06-45\n",
      "2024-11-27 20:08:01,357\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/JCA/ray_results/gradient_boosting_tuning' in 0.0150s.\n",
      "2024-11-27 20:08:01,375\tINFO tune.py:1041 -- Total run time: 75.49 seconds (75.44 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m 2024/11/27 20:06:53 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m     - be included in your $PATH\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m     - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m \n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m \n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m This initial message can be silenced or aggravated in the future by setting the\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m     - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m     - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m     - error|e|exception|raise|r|2: for a raised exception\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m \n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m Example:\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m \n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m 2024/11/27 20:08:01 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: D:\\TEMP\\Temp\\tmpf7qx510s\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.5.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[36m(train_gbc_sm pid=34368)\u001b[0m 2024/11/27 20:08:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "            'learning_rate': tune.uniform(0.001, 0.1),\n",
    "            'max_depth': tune.randint(1, 15),\n",
    "            'max_features': tune.uniform(0, 1),\n",
    "            'min_samples_leaf': tune.randint(10, 15),\n",
    "            'min_samples_split': tune.randint(15, 18),\n",
    "            'n_estimators':  tune.randint(300, 500),\n",
    "            'subsample':  tune.uniform(0.1, 0.9)\n",
    "        }\n",
    "\n",
    "search_algo = OptunaSearch()\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "aritifact_path = \"model\"\n",
    "def train_gbc_sm(config: dict, X_train: pd.DataFrame, y_train: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "    `setup_mlflow()`:\n",
    "        - uploading to an existing experiment works if initially created with `setup_mlflow`\n",
    "        - creating \"run groups\" inside an experiment does not work\n",
    "    \"\"\"\n",
    "\n",
    "    setup_mlflow(config)\n",
    "\n",
    "    mlflow.set_tracking_uri(uri)\n",
    "    mlflow.set_experiment(experiment_name='DHI_Classifiers_Ray_sm1') #\n",
    "    mlflow.log_params(config)\n",
    "\n",
    "    X_train, y_train = X_train, y_train\n",
    "    gbc_clf = GradientBoostingClassifier(\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_depth=config['max_depth'],\n",
    "        max_features=config['max_features'],\n",
    "        min_samples_leaf=config['min_samples_leaf'],\n",
    "        min_samples_split=config['min_samples_split'],\n",
    "        n_estimators=config['n_estimators'],\n",
    "        subsample=config['subsample']\n",
    "    )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    y_pred_proba = cross_val_predict(estimator=gbc_clf, \n",
    "                                   X= X_train, \n",
    "                                   y = y_train.values.ravel(), \n",
    "                                   cv=skf, \n",
    "                                   method='predict_proba')\n",
    "    \n",
    "    log_loss_score = log_loss(y_true=y_train, y_pred=y_pred_proba)\n",
    "    y_pred_class = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "    metrics = {\n",
    "            'log_loss' : log_loss_score,\n",
    "            'accuracy': accuracy_score(y_train, y_pred_class),\n",
    "            'f1_score': f1_score(y_train, y_pred_class, average='macro'),\n",
    "            'recall_score': recall_score(y_train, y_pred_class, average='macro'),\n",
    "            'precision': precision_score(y_train, y_pred_class, average='macro')\n",
    "        }\n",
    "\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.sklearn.log_model(gbc_clf, artifact_path=aritifact_path)\n",
    "\n",
    "    train.report(\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "\n",
    "train_gbc_gpu_sm = tune.with_resources(train_gbc_sm, {'cpu':4, 'gpu':1})\n",
    "def tune_to_mlflow_sm(mlflow_uri=uri):\n",
    "    mlflow.set_tracking_uri(mlflow_uri)\n",
    "    mlflow.set_experiment(experiment_name='DHI_Classifiers_Ray_sm1')\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_parameters(trainable=train_gbc_gpu_sm, X_train=X_train, y_train=y_train),\n",
    "        param_space=search_space,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=1, #10\n",
    "            metric='log_loss', \n",
    "            mode='min',\n",
    "            search_alg=search_algo,\n",
    "            scheduler=scheduler),\n",
    "        run_config=train.RunConfig(\n",
    "            name=\"gradient_boosting_tuning\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "res2 = tune_to_mlflow_sm(mlflow_uri=uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mlflow.start_run()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/328477487984719548', creation_time=1732709281478, experiment_id='328477487984719548', last_update_time=1732709281478, lifecycle_stage='active', name='DHI_CLF_ray_opt', tags={}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_id = get_or_create_experiment(\"DHI_CLF_ray_opt\")\n",
    "mlflow.set_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "            'learning_rate': tune.uniform(0.001, 0.1),\n",
    "            'max_depth': tune.randint(1, 15),\n",
    "            'max_features': tune.uniform(0, 1),\n",
    "            'min_samples_leaf': tune.randint(10, 15),\n",
    "            'min_samples_split': tune.randint(15, 18),\n",
    "            'n_estimators':  tune.randint(300, 500),\n",
    "            'subsample':  tune.uniform(0.1, 0.9)\n",
    "        }\n",
    "search_algo = OptunaSearch()\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=2\n",
    ")\n",
    "aritifact_path = \"model\"\n",
    "\n",
    "def train_gbc_mlflow(config: dict, X_train: pd.DataFrame, y_train: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "    `mlflow.start_run`\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='test_01'):\n",
    "        X_train, y_train = X_train, y_train\n",
    "        gbc_clf = GradientBoostingClassifier(\n",
    "            learning_rate=config['learning_rate'],\n",
    "            max_depth=config['max_depth'],\n",
    "            max_features=config['max_features'],\n",
    "            min_samples_leaf=config['min_samples_leaf'],\n",
    "            min_samples_split=config['min_samples_split'],\n",
    "            n_estimators=config['n_estimators'],\n",
    "            subsample=config['subsample']\n",
    "        )\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        y_pred_proba = cross_val_predict(estimator=gbc_clf, \n",
    "                                    X= X_train, \n",
    "                                    y = y_train.values.ravel(), \n",
    "                                    cv=skf, \n",
    "                                    method='predict_proba')\n",
    "        \n",
    "        log_loss_score = log_loss(y_true=y_train, y_pred=y_pred_proba)\n",
    "        y_pred_class = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "        metrics = {\n",
    "                'log_loss' : log_loss_score,\n",
    "                'accuracy': accuracy_score(y_train, y_pred_class),\n",
    "                'f1_score': f1_score(y_train, y_pred_class, average='macro'),\n",
    "                'recall_score': recall_score(y_train, y_pred_class, average='macro'),\n",
    "                'precision': precision_score(y_train, y_pred_class, average='macro')\n",
    "            }\n",
    "\n",
    "        mlflow.log_params(config)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.sklearn.log_model(gbc_clf, \n",
    "                                 artifact_path=aritifact_path,\n",
    "                                 input_example=X_train.iloc[[0]])\n",
    "\n",
    "        train.report(\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        return log_loss_score\n",
    "    \n",
    "train_gbc_gpu_mlflow = tune.with_resources(train_gbc_mlflow, {'cpu':4, 'gpu':1})\n",
    "def tune_to_mlflow_mlflow(mlflow_uri=uri):\n",
    "    mlflow_uri=mlflow_uri\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_parameters(trainable=train_gbc_gpu_mlflow, X_train=X_train, y_train=y_train),\n",
    "        param_space=search_space,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=1, #10\n",
    "            metric='log_loss', \n",
    "            mode='min',\n",
    "            search_alg=search_algo,\n",
    "            scheduler=scheduler),\n",
    "        run_config=train.RunConfig(\n",
    "            name=\"gradient_boosting_tuning\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "#tune_to_mlflow_mlflow(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
