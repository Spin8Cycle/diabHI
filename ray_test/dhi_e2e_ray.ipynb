{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "git.refresh(r'C:\\Program Files\\Git\\bin\\git.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = r'D:\\Notes\\git local repo\\Diabetes Health Indicator\\data\\diabetesHI\\diabetes_012_health_indicators_BRFSS2015.csv'\n",
    "\n",
    "def sampler_pipeline(df:pd.DataFrame,label:str ,sampler:str) -> pd.DataFrame:\n",
    "    X = df.drop(label, axis=1)\n",
    "    y = df[[label]]\n",
    "    \n",
    "    sampling_list = ['No resampling (Original Data)', 'imblearn: RandomUnderSampler', 'imblearn: SMOTE']\n",
    "    if sampler == sampling_list[0]:\n",
    "        return X, y\n",
    "    elif sampler == sampling_list[1]:\n",
    "        ran_down = RandomUnderSampler(random_state=42)\n",
    "        X_dws, y_dws = ran_down.fit_resample(X,y)\n",
    "        return X_dws, y_dws\n",
    "    elif sampler == sampling_list[2]:\n",
    "        smote_ups = SMOTE(random_state=42)\n",
    "        X_ups, y_ups = smote_ups.fit_resample(X, y)\n",
    "        return X_ups, y_ups\n",
    "    else:\n",
    "        raise IndexError(\"Sampler is not on the pre-defined Sampling List:\\\n",
    "                          ['No resampling (Original Data)', 'imblearn: RandomUnderSampler', 'imblearn: SMOTE']\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_url)\n",
    "X, y = sampler_pipeline(df=df,label='Diabetes_012' ,sampler='imblearn: SMOTE')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, recall_score, precision_score\n",
    "from ray.air.integrations.mlflow import setup_mlflow, MLflowLoggerCallback\n",
    "\n",
    "import mlflow.sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-27 20:39:42</td></tr>\n",
       "<tr><td>Running for: </td><td>00:19:15.75        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.9/15.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None<br>Logical resource usage: 4.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  max_features</th><th style=\"text-align: right;\">  min_samples_leaf</th><th style=\"text-align: right;\">  min_samples_split</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  subsample</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_gbc_sm_b11afb12</td><td>RUNNING </td><td>127.0.0.1:36432</td><td style=\"text-align: right;\">     0.00651972</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">      0.829963</td><td style=\"text-align: right;\">                13</td><td style=\"text-align: right;\">                 16</td><td style=\"text-align: right;\">           469</td><td style=\"text-align: right;\">   0.365664</td></tr>\n",
       "<tr><td>train_gbc_sm_b80f7613</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">     0.0299361 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      0.336513</td><td style=\"text-align: right;\">                11</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">           395</td><td style=\"text-align: right;\">   0.711782</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 20:20:26,305\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_19-25-59_109538_29288\\artifacts\\2024-11-27_20-20-24\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b11afb12_1_learning_rate=0.0065,max_depth=7,max_features=0.8300,min_samples_leaf=13,min_samples_split=16,n_estimators_2024-11-27_20-20-26\n",
      "2024-11-27 20:20:26,315\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_19-25-59_109538_29288\\artifacts\\2024-11-27_20-20-24\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b11afb12_1_learning_rate=0.0065,max_depth=7,max_features=0.8300,min_samples_leaf=13,min_samples_split=16,n_estimators_2024-11-27_20-20-26\n",
      "2024-11-27 20:20:33,200\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_19-25-59_109538_29288\\artifacts\\2024-11-27_20-20-24\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b11afb12_1_learning_rate=0.0065,max_depth=7,max_features=0.8300,min_samples_leaf=13,min_samples_split=16,n_estimators_2024-11-27_20-20-26\n",
      "2024-11-27 20:20:33,202\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_19-25-59_109538_29288\\artifacts\\2024-11-27_20-20-24\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b11afb12_1_learning_rate=0.0065,max_depth=7,max_features=0.8300,min_samples_leaf=13,min_samples_split=16,n_estimators_2024-11-27_20-20-26\n",
      "2024-11-27 20:20:33,217\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_19-25-59_109538_29288\\artifacts\\2024-11-27_20-20-24\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b80f7613_2_learning_rate=0.0299,max_depth=4,max_features=0.3365,min_samples_leaf=11,min_samples_split=15,n_estimators_2024-11-27_20-20-33\n",
      "2024-11-27 20:20:33,222\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-27_19-25-59_109538_29288\\artifacts\\2024-11-27_20-20-24\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_b80f7613_2_learning_rate=0.0299,max_depth=4,max_features=0.3365,min_samples_leaf=11,min_samples_split=15,n_estimators_2024-11-27_20-20-33\n",
      "2024-11-27 20:39:42,025\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-11-27 20:39:42,045\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/JCA/ray_results/gradient_boosting_tuning' in 0.0175s.\n",
      "2024-11-27 20:39:52,200\tINFO tune.py:1041 -- Total run time: 1168.11 seconds (1155.73 seconds for the tuning loop).\n",
      "2024-11-27 20:39:52,202\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"C:/Users/JCA/ray_results/gradient_boosting_tuning\", trainable=...)\n",
      "2024-11-27 20:39:52,217\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- train_gbc_sm_b80f7613: FileNotFoundError('Could not fetch metrics for train_gbc_sm_b80f7613: both result.json and progress.csv were not found at C:/Users/JCA/ray_results/gradient_boosting_tuning/train_gbc_sm_b80f7613_2_learning_rate=0.0299,max_depth=4,max_features=0.3365,min_samples_leaf=11,min_samples_split=15,n_estimators_2024-11-27_20-20-33')\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "            'learning_rate': tune.uniform(0.001, 0.1),\n",
    "            'max_depth': tune.randint(1, 15),\n",
    "            'max_features': tune.uniform(0, 1),\n",
    "            'min_samples_leaf': tune.randint(10, 15),\n",
    "            'min_samples_split': tune.randint(15, 18),\n",
    "            'n_estimators':  tune.randint(300, 500),\n",
    "            'subsample':  tune.uniform(0.1, 0.9)\n",
    "        }\n",
    "\n",
    "search_algo = OptunaSearch()\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "artifact_path='model'\n",
    "exp_name = 'DHI_Classifiers_RayOpt'\n",
    "uri = \"http://localhost:5000\"\n",
    "\n",
    "\n",
    "def train_gbc_sm(config: dict, X_train: pd.DataFrame, y_train: pd.DataFrame):\n",
    "\n",
    "    X_train, y_train = X_train, y_train\n",
    "    gbc_clf = GradientBoostingClassifier(\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_depth=config['max_depth'],\n",
    "        max_features=config['max_features'],\n",
    "        min_samples_leaf=config['min_samples_leaf'],\n",
    "        min_samples_split=config['min_samples_split'],\n",
    "        n_estimators=config['n_estimators'],\n",
    "        subsample=config['subsample']\n",
    "    )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    y_pred_proba = cross_val_predict(estimator=gbc_clf, \n",
    "                                   X= X_train, \n",
    "                                   y = y_train.values.ravel(), \n",
    "                                   cv=skf, \n",
    "                                   method='predict_proba')\n",
    "    \n",
    "    log_loss_score = log_loss(y_true=y_train, y_pred=y_pred_proba)\n",
    "    y_pred_class = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "    metrics = {\n",
    "            'log_loss' : log_loss_score,\n",
    "            'accuracy': accuracy_score(y_train, y_pred_class),\n",
    "            'f1_score': f1_score(y_train, y_pred_class, average='macro'),\n",
    "            'recall_score': recall_score(y_train, y_pred_class, average='macro'),\n",
    "            'precision': precision_score(y_train, y_pred_class, average='macro')\n",
    "        }\n",
    "\n",
    "    train.report(\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run(run_name=exp_name):\n",
    "            # Log the model\n",
    "        mlflow.sklearn.log_model(gbc_clf, artifact_path=artifact_path)\n",
    "\n",
    "train_gbc_gpu_sm = tune.with_resources(train_gbc_sm, {'cpu':4, 'gpu':1})\n",
    "mlflow_cb = MLflowLoggerCallback(\n",
    "                    tracking_uri=uri,\n",
    "                    experiment_name=exp_name,\n",
    "                    save_artifact=True,\n",
    "                )\n",
    "\n",
    "def tune_to_mlflow_sm():\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_parameters(trainable=train_gbc_gpu_sm, X_train=X_train, y_train=y_train),\n",
    "        param_space=search_space,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=2, #10\n",
    "            metric='log_loss', \n",
    "            mode='min',\n",
    "            search_alg=search_algo,\n",
    "            scheduler=scheduler),\n",
    "        run_config=train.RunConfig(\n",
    "            name=\"gradient_boosting_tuning\",\n",
    "            callbacks=[mlflow_cb]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "result = tune_to_mlflow_sm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
