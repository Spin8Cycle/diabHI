{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = r'D:\\Notes\\git local repo\\Diabetes Health Indicator\\data\\diabetesHI\\diabetes_012_health_indicators_BRFSS2015.csv'\n",
    "\n",
    "def sampler_pipeline(df:pd.DataFrame,label:str ,sampler:str) -> pd.DataFrame:\n",
    "    X = df.drop(label, axis=1)\n",
    "    y = df[[label]]\n",
    "    \n",
    "    sampling_list = ['No resampling (Original Data)', 'imblearn: RandomUnderSampler', 'imblearn: SMOTE']\n",
    "    if sampler == sampling_list[0]:\n",
    "        return X, y\n",
    "    elif sampler == sampling_list[1]:\n",
    "        ran_down = RandomUnderSampler(random_state=42)\n",
    "        X_dws, y_dws = ran_down.fit_resample(X,y)\n",
    "        return X_dws, y_dws\n",
    "    elif sampler == sampling_list[2]:\n",
    "        smote_ups = SMOTE(random_state=42)\n",
    "        X_ups, y_ups = smote_ups.fit_resample(X, y)\n",
    "        return X_ups, y_ups\n",
    "    else:\n",
    "        raise IndexError(\"Sampler is not on the pre-defined Sampling List:\\\n",
    "                          ['No resampling (Original Data)', 'imblearn: RandomUnderSampler', 'imblearn: SMOTE']\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_url)\n",
    "X, y = sampler_pipeline(df=df,label='Diabetes_012' ,sampler='imblearn: SMOTE')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, recall_score, precision_score\n",
    "from ray.air.integrations.mlflow import setup_mlflow\n",
    "\n",
    "import mlflow.sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-25 20:21:26</td></tr>\n",
       "<tr><td>Running for: </td><td>00:14:40.60        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.8/15.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None<br>Logical resource usage: 4.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  max_features</th><th style=\"text-align: right;\">  min_samples_leaf</th><th style=\"text-align: right;\">  min_samples_split</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  subsample</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_gbc_sm_c5c9f4b9</td><td>RUNNING </td><td>127.0.0.1:28240</td><td style=\"text-align: right;\">      0.0455783</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">       0.85682</td><td style=\"text-align: right;\">                12</td><td style=\"text-align: right;\">                 17</td><td style=\"text-align: right;\">           438</td><td style=\"text-align: right;\">   0.724409</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 20:06:46,327\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-25_20-06-32_604401_32784\\artifacts\\2024-11-25_20-06-41\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_c5c9f4b9_1_learning_rate=0.0456,max_depth=10,max_features=0.8568,min_samples_leaf=12,min_samples_split=17,n_estimator_2024-11-25_20-06-46\n",
      "2024-11-25 20:06:46,336\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-25_20-06-32_604401_32784\\artifacts\\2024-11-25_20-06-41\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_c5c9f4b9_1_learning_rate=0.0456,max_depth=10,max_features=0.8568,min_samples_leaf=12,min_samples_split=17,n_estimator_2024-11-25_20-06-46\n",
      "2024-11-25 20:06:53,364\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-25_20-06-32_604401_32784\\artifacts\\2024-11-25_20-06-41\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_c5c9f4b9_1_learning_rate=0.0456,max_depth=10,max_features=0.8568,min_samples_leaf=12,min_samples_split=17,n_estimator_2024-11-25_20-06-46\n",
      "2024-11-25 20:06:53,365\tWARNING trial.py:647 -- The path to the trial log directory is too long (max length: 260. Consider using `trial_dirname_creator` to shorten the path. Path: D:\\TEMP\\Temp\\ray\\session_2024-11-25_20-06-32_604401_32784\\artifacts\\2024-11-25_20-06-41\\gradient_boosting_tuning\\driver_artifacts\\train_gbc_sm_c5c9f4b9_1_learning_rate=0.0456,max_depth=10,max_features=0.8568,min_samples_leaf=12,min_samples_split=17,n_estimator_2024-11-25_20-06-46\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m 2024/11/25 20:06:53 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m The git executable must be specified in one of the following ways:\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m     - be included in your $PATH\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m     - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m     - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m \n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m All git commands will error until this is rectified.\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m \n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m This initial message can be silenced or aggravated in the future by setting the\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m     - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m     - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m     - error|e|exception|raise|r|2: for a raised exception\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m \n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m Example:\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m     export GIT_PYTHON_REFRESH=quiet\n",
      "\u001b[36m(train_gbc_sm pid=28240)\u001b[0m \n",
      "2024-11-25 20:21:26,904\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-11-25 20:21:26,918\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/JCA/ray_results/gradient_boosting_tuning' in 0.0123s.\n",
      "2024-11-25 20:21:37,033\tINFO tune.py:1041 -- Total run time: 895.58 seconds (880.58 seconds for the tuning loop).\n",
      "2024-11-25 20:21:37,035\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"C:/Users/JCA/ray_results/gradient_boosting_tuning\", trainable=...)\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "            'learning_rate': tune.uniform(0.001, 0.1),\n",
    "            'max_depth': tune.randint(1, 15),\n",
    "            'max_features': tune.uniform(0, 1),\n",
    "            'min_samples_leaf': tune.randint(10, 15),\n",
    "            'min_samples_split': tune.randint(15, 18),\n",
    "            'n_estimators':  tune.randint(300, 500),\n",
    "            'subsample':  tune.uniform(0.1, 0.9)\n",
    "        }\n",
    "\n",
    "search_algo = OptunaSearch()\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "artifact_path='model'\n",
    "exp_name = 'DHI_Classifiers_RayOpt'\n",
    "uri = \"http://localhost:5000\"\n",
    "\n",
    "mlflow.set_tracking_uri(uri)\n",
    "mlflow.set_experiment(experiment_name=exp_name)\n",
    "\n",
    "def train_gbc_sm(config: dict, X_train: pd.DataFrame, y_train: pd.DataFrame):\n",
    "    setup_mlflow(config)\n",
    "    mlflow.log_params(config)\n",
    "\n",
    "    X_train, y_train = X_train, y_train\n",
    "    gbc_clf = GradientBoostingClassifier(\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_depth=config['max_depth'],\n",
    "        max_features=config['max_features'],\n",
    "        min_samples_leaf=config['min_samples_leaf'],\n",
    "        min_samples_split=config['min_samples_split'],\n",
    "        n_estimators=config['n_estimators'],\n",
    "        subsample=config['subsample']\n",
    "    )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    y_pred_proba = cross_val_predict(estimator=gbc_clf, \n",
    "                                   X= X_train, \n",
    "                                   y = y_train.values.ravel(), \n",
    "                                   cv=skf, \n",
    "                                   method='predict_proba')\n",
    "    \n",
    "    log_loss_score = log_loss(y_true=y_train, y_pred=y_pred_proba)\n",
    "    y_pred_class = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "    metrics = {\n",
    "            'log_loss' : log_loss_score,\n",
    "            'accuracy': accuracy_score(y_train, y_pred_class),\n",
    "            'f1_score': f1_score(y_train, y_pred_class, average='macro'),\n",
    "            'recall_score': recall_score(y_train, y_pred_class, average='macro'),\n",
    "            'precision': precision_score(y_train, y_pred_class, average='macro')\n",
    "        }\n",
    "\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.sklearn.log_model(gbc_clf, artifact_path=artifact_path)\n",
    "\n",
    "    train.report(\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "train_gbc_gpu_sm = tune.with_resources(train_gbc_sm, {'cpu':4, 'gpu':1})\n",
    "def tune_to_mlflow_sm(mlflow_uri=uri):\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_parameters(trainable=train_gbc_gpu_sm, X_train=X_train, y_train=y_train),\n",
    "        param_space=search_space,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=1, #10\n",
    "            metric='log_loss', \n",
    "            mode='min',\n",
    "            search_alg=search_algo,\n",
    "            scheduler=scheduler),\n",
    "        run_config=train.RunConfig(\n",
    "            name=\"gradient_boosting_tuning\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "result = tune_to_mlflow_sm(mlflow_uri=uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
